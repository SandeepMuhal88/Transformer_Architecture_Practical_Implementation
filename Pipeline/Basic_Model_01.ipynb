{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0f65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd91f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9755d3be7f0243df9431c06779bda037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project-to-learn\\.virpytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sande\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11f33c20fed4d7a8db68410880283a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1b39c16dd74038972c6b84bf3ed676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998359680175781}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "result = classifier(\"Transformers are amazing for NLP!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842b647",
   "metadata": {},
   "source": [
    "### 2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e903442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Once upon a time, there was a coder named Sandeep who'd taken over the code that made an application possible. Sandeep was looking for code to replace the old one, which was just a few line commands. He wanted to learn the first few lines of the program, but he didn't really know what he was doing. He had a few more lines to learn, but he didn't have any ideas where he could get the information. When he tried to figure out what he was doing, he found out he was doing something that was not the original. He was creating a more complex program that would be faster and faster:\\n\\nTo get this to work, he had to pass all the lines of the program into a variable called program. He then looked up the file called program.c in the same directory that Sandeep was working on. He found out that the program was using the default command line arguments. This means that if the program was created with a $USER variable, it would be built into whatever file Sandeep was working on. He wanted to find the $USER variable, but he couldn't find it in any other folder on the computer, so he tried a different approach. He tried to use a different command line argument, and found out that the $USER variable did not exist\"}]\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Generation\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "results=gen(\"Once upon a time, there was a coder named Sandeep\", max_length=40)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbbb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Your max_length is set to 142, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Exception ignored in: <function tqdm.__del__ at 0x00000153AA36FA30>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Project-to-learn\\.virpytorch\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"d:\\Project-to-learn\\.virpytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The transformer architecture was first proposed in 2017 by Vaswani et al. It revolutionized NLP . It revolutionizes NLP. Transformer architecture was proposed in 2018 . It is the first time a transformer architecture has been proposed in the U.S. It was proposed as a way to improve NLP theory .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"The transformer architecture was first proposed in 2017 by Vaswani et al. It revolutionized NLP.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virpytorch (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
