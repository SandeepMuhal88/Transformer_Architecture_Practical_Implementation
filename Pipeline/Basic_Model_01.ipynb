{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59388620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device=0\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd91f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project-to-learn\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998359680175781}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "result = classifier(\"Transformers are amazing for NLP!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842b647",
   "metadata": {},
   "source": [
    "### 2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e903442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Once upon a time, there was a coder named Sandeep who'd taken over the code that made an application possible. Sandeep was looking for code to replace the old one, which was just a few line commands. He wanted to learn the first few lines of the program, but he didn't really know what he was doing. He had a few more lines to learn, but he didn't have any ideas where he could get the information. When he tried to figure out what he was doing, he found out he was doing something that was not the original. He was creating a more complex program that would be faster and faster:\\n\\nTo get this to work, he had to pass all the lines of the program into a variable called program. He then looked up the file called program.c in the same directory that Sandeep was working on. He found out that the program was using the default command line arguments. This means that if the program was created with a $USER variable, it would be built into whatever file Sandeep was working on. He wanted to find the $USER variable, but he couldn't find it in any other folder on the computer, so he tried a different approach. He tried to use a different command line argument, and found out that the $USER variable did not exist\"}]\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Generation\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "results=gen(\"Once upon a time, there was a coder named Sandeep\", max_length=40)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbbb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Your max_length is set to 142, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Exception ignored in: <function tqdm.__del__ at 0x00000153AA36FA30>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Project-to-learn\\.virpytorch\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"d:\\Project-to-learn\\.virpytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The transformer architecture was first proposed in 2017 by Vaswani et al. It revolutionized NLP . It revolutionizes NLP. Transformer architecture was proposed in 2018 . It is the first time a transformer architecture has been proposed in the U.S. It was proposed as a way to improve NLP theory .'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"The transformer architecture was first proposed in 2017 by Vaswani et al. It revolutionized NLP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe0e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "d:\\Project-to-learn\\.env\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.613925576210022,\n",
       " 'start': 32,\n",
       " 'end': 50,\n",
       " 'answer': 'Google researchers'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\", device=0)\n",
    "qa({\n",
    "    \"question\": \"Who introduced Transformers?\",\n",
    "    \"context\": \"Transformers were introduced by Google researchers in 2017.\"\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
